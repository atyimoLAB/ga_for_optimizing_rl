{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b940478",
   "metadata": {},
   "source": [
    "# Linkage manual review dataset — Elasticsearch candidates with controlled rank sampling\n",
    "\n",
    "Este notebook cria, a partir de um DataFrame Spark com atributos de linkage, uma coluna:\n",
    "\n",
    "- `es_candidates`: lista (array) com até **N** candidatos retornados do Elasticsearch (via `_msearch`)\n",
    "- `target_pos`: posição-alvo sorteada por linha segundo uma **distribuição configurada em YAML**\n",
    "- `es_candidate`: candidato escolhido de `es_candidates[target_pos-1]` com fallback configurável\n",
    "\n",
    "Pré-requisitos:\n",
    "- Cluster Spark disponível\n",
    "- Acesso ao Elasticsearch (rede + credenciais se necessário)\n",
    "- Pacotes Python: `pyyaml`, `requests`\n",
    "\n",
    "> Observação: a abordagem usa `_msearch` para reduzir o overhead (lote por partição).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9793f2b",
   "metadata": {},
   "source": [
    "## 1) Configuração YAML (exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste o caminho do YAML conforme seu ambiente (Databricks / Jupyter / filesystem)\n",
    "yaml_path = \"/dbfs/FileStore/linkage_cfg.yaml\"  # exemplo Databricks\n",
    "# yaml_path = \"linkage_cfg.yaml\"               # exemplo local\n",
    "print(\"YAML path:\", yaml_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c81d2a",
   "metadata": {},
   "source": [
    "Exemplo de `linkage_cfg.yaml`:\n",
    "\n",
    "```yaml\n",
    "elasticsearch:\n",
    "  hosts: [\"https://meu-es:9200\"]\n",
    "  index: \"sinasc_index\"\n",
    "  auth:\n",
    "    user_env: \"ES_USER\"\n",
    "    pass_env: \"ES_PASS\"\n",
    "  tls:\n",
    "    verify_certs: false\n",
    "\n",
    "query:\n",
    "  max_candidates: 10\n",
    "  source_fields: [\"nome\", \"nome_mae\", \"dt_nasc\", \"sexo\", \"municipio_res\"]\n",
    "  fields:\n",
    "    - col: \"nome\"\n",
    "      es_field: \"nome\"\n",
    "      type: \"match\"\n",
    "      fuzziness: \"AUTO\"\n",
    "      boost: 3.0\n",
    "      operator: \"AND\"\n",
    "    - col: \"nome_mae\"\n",
    "      es_field: \"nome_mae\"\n",
    "      type: \"match\"\n",
    "      fuzziness: \"AUTO\"\n",
    "      boost: 2.0\n",
    "      operator: \"AND\"\n",
    "    - col: \"dt_nasc\"\n",
    "      es_field: \"dt_nasc\"\n",
    "      type: \"term\"\n",
    "      boost: 2.0\n",
    "    - col: \"sexo\"\n",
    "      es_field: \"sexo\"\n",
    "      type: \"term\"\n",
    "      boost: 1.0\n",
    "    - col: \"municipio_res\"\n",
    "      es_field: \"municipio_res\"\n",
    "      type: \"term\"\n",
    "      boost: 1.0\n",
    "\n",
    "sampling:\n",
    "  position_distribution_pct:\n",
    "    \"1\": 55\n",
    "    \"2\": 20\n",
    "    \"3\": 10\n",
    "    \"4\": 7\n",
    "    \"5\": 4\n",
    "    \"6\": 2\n",
    "    \"7\": 1\n",
    "    \"8\": 1\n",
    "  seed: 42\n",
    "  fallback_when_short:\n",
    "    mode: \"last_available\"   # options: null | last_available | first_available\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d633c1",
   "metadata": {},
   "source": [
    "## 2) Carregar YAML e validar distribuição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1746220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "cfg = yaml.safe_load(open(yaml_path, \"r\"))\n",
    "\n",
    "dist = cfg[\"sampling\"][\"position_distribution_pct\"]\n",
    "seed = int(cfg[\"sampling\"].get(\"seed\", 42))\n",
    "\n",
    "# valida soma = 100\n",
    "s = sum(int(v) for v in dist.values())\n",
    "assert s == 100, f\"Distribuição precisa somar 100 (atual: {s})\"\n",
    "\n",
    "max_candidates = int(cfg[\"query\"][\"max_candidates\"])\n",
    "fallback_mode = cfg[\"sampling\"][\"fallback_when_short\"][\"mode\"]\n",
    "\n",
    "print(\"OK! max_candidates =\", max_candidates)\n",
    "print(\"fallback_mode =\", fallback_mode)\n",
    "print(\"seed =\", seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2156edf",
   "metadata": {},
   "source": [
    "## 3) DataFrame de entrada (exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8502d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esperado: df com colunas (ajuste aos seus nomes reais)\n",
    "# - nome, nome_mae, dt_nasc, sexo, municipio_res\n",
    "# Exemplos:\n",
    "# df = spark.table(\"minha_tabela_linkage_features\")\n",
    "# df = spark.read.parquet(\"...\")\n",
    "\n",
    "# Para evitar erro se você rodar sem df definido:\n",
    "try:\n",
    "    df\n",
    "    print(\"df já existe. Colunas:\", df.columns)\n",
    "except NameError:\n",
    "    print(\"Defina o DataFrame 'df' antes de continuar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760ff5d",
   "metadata": {},
   "source": [
    "## 4) Sortear a posição-alvo `target_pos` conforme o YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "items = sorted([(int(k), int(v)) for k, v in dist.items()], key=lambda x: x[0])\n",
    "\n",
    "# acumulados em [0,1]\n",
    "cum = []\n",
    "acc = 0\n",
    "for pos, pct in items:\n",
    "    acc += pct\n",
    "    cum.append((pos, acc / 100.0))\n",
    "\n",
    "u = F.rand(seed)\n",
    "\n",
    "case_expr = None\n",
    "for pos, thr in cum:\n",
    "    cond = (u <= F.lit(thr))\n",
    "    case_expr = F.when(cond, F.lit(pos)) if case_expr is None else case_expr.when(cond, F.lit(pos))\n",
    "case_expr = case_expr.otherwise(F.lit(items[-1][0]))\n",
    "\n",
    "df2 = df.withColumn(\"target_pos\", case_expr.cast(\"int\"))\n",
    "df2.select(\"target_pos\").groupBy(\"target_pos\").count().orderBy(\"target_pos\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b52a21",
   "metadata": {},
   "source": [
    "## 5) Buscar candidatos no Elasticsearch via `_msearch` (por partição)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd707f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Iterator, Dict, Any\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "es_hosts = cfg[\"elasticsearch\"][\"hosts\"]\n",
    "es_index = cfg[\"elasticsearch\"][\"index\"]\n",
    "verify = bool(cfg[\"elasticsearch\"].get(\"tls\", {}).get(\"verify_certs\", True))\n",
    "\n",
    "user = os.getenv(cfg[\"elasticsearch\"][\"auth\"][\"user_env\"], \"\")\n",
    "pwd  = os.getenv(cfg[\"elasticsearch\"][\"auth\"][\"pass_env\"], \"\")\n",
    "\n",
    "fields_cfg = cfg[\"query\"][\"fields\"]\n",
    "src_fields = cfg[\"query\"].get(\"source_fields\", [])\n",
    "\n",
    "def build_es_query(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    should = []\n",
    "    for f in fields_cfg:\n",
    "        col = f[\"col\"]\n",
    "        val = row.get(col)\n",
    "\n",
    "        if val is None or (isinstance(val, str) and val.strip() == \"\"):\n",
    "            continue\n",
    "\n",
    "        qtype = f[\"type\"]\n",
    "        es_field = f[\"es_field\"]\n",
    "        boost = float(f.get(\"boost\", 1.0))\n",
    "\n",
    "        if qtype == \"match\":\n",
    "            clause = {\n",
    "                \"match\": {\n",
    "                    es_field: {\n",
    "                        \"query\": val,\n",
    "                        \"boost\": boost,\n",
    "                        \"operator\": f.get(\"operator\", \"OR\"),\n",
    "                        \"fuzziness\": f.get(\"fuzziness\", \"AUTO\"),\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        elif qtype == \"term\":\n",
    "            clause = {\"term\": {es_field: {\"value\": val, \"boost\": boost}}}\n",
    "        else:\n",
    "            raise ValueError(f\"Tipo não suportado: {qtype}\")\n",
    "\n",
    "        should.append(clause)\n",
    "\n",
    "    return {\n",
    "        \"size\": max_candidates,\n",
    "        \"_source\": src_fields,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": should,\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "cand_schema = T.ArrayType(T.StructType([\n",
    "    T.StructField(\"es_id\", T.StringType(), True),\n",
    "    T.StructField(\"es_score\", T.DoubleType(), True),\n",
    "    T.StructField(\"es_source\", T.StringType(), True),  # JSON compactado (opcional)\n",
    "]))\n",
    "\n",
    "out_schema = T.StructType(df2.schema.fields + [\n",
    "    T.StructField(\"es_candidates\", cand_schema, True),\n",
    "])\n",
    "\n",
    "def fetch_partition(rows: Iterator[Any]) -> Iterator[Any]:\n",
    "    rows_list = list(rows)\n",
    "    if not rows_list:\n",
    "        return iter([])\n",
    "\n",
    "    ndjson_lines = []\n",
    "    for r in rows_list:\n",
    "        d = r.asDict(recursive=True)\n",
    "        header = {\"index\": es_index}\n",
    "        query  = build_es_query(d)\n",
    "        ndjson_lines.append(json.dumps(header))\n",
    "        ndjson_lines.append(json.dumps(query))\n",
    "\n",
    "    body = \"\\n\".join(ndjson_lines) + \"\\n\"\n",
    "\n",
    "    base = es_hosts[0].rstrip(\"/\")\n",
    "    url = f\"{base}/_msearch\"\n",
    "\n",
    "    auth = (user, pwd) if (user or pwd) else None\n",
    "\n",
    "    resp = requests.post(\n",
    "        url,\n",
    "        data=body,\n",
    "        headers={\"Content-Type\": \"application/x-ndjson\"},\n",
    "        auth=auth,\n",
    "        verify=verify,\n",
    "        timeout=60\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    payload = resp.json()\n",
    "\n",
    "    responses = payload.get(\"responses\", [])\n",
    "    if len(responses) != len(rows_list):\n",
    "        raise RuntimeError(f\"Resposta do ES desalinhada: {len(responses)} != {len(rows_list)}\")\n",
    "\n",
    "    out = []\n",
    "    for r, pr in zip(rows_list, responses):\n",
    "        hits = pr.get(\"hits\", {}).get(\"hits\", [])\n",
    "        cands = []\n",
    "        for h in hits:\n",
    "            cands.append({\n",
    "                \"es_id\": h.get(\"_id\"),\n",
    "                \"es_score\": float(h.get(\"_score\") or 0.0),\n",
    "                \"es_source\": json.dumps(h.get(\"_source\", {}), ensure_ascii=False),\n",
    "            })\n",
    "        out.append(tuple(list(r) + [cands]))\n",
    "\n",
    "    return iter(out)\n",
    "\n",
    "df3 = df2.rdd.mapPartitions(fetch_partition).toDF(schema=out_schema)\n",
    "\n",
    "df3.select(\"target_pos\", F.size(\"es_candidates\").alias(\"n_cands\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12d3e7",
   "metadata": {},
   "source": [
    "## 6) Criar `es_candidate` escolhendo a posição sorteada (com fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "cand_struct = T.StructType([\n",
    "    T.StructField(\"es_id\", T.StringType(), True),\n",
    "    T.StructField(\"es_score\", T.DoubleType(), True),\n",
    "    T.StructField(\"es_source\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "def pick_candidate(cands, target_pos: int):\n",
    "    if cands is None or len(cands) == 0:\n",
    "        return None\n",
    "    idx = (target_pos or 1) - 1\n",
    "    if idx < len(cands):\n",
    "        return cands[idx]\n",
    "    if fallback_mode == \"last_available\":\n",
    "        return cands[-1]\n",
    "    if fallback_mode == \"first_available\":\n",
    "        return cands[0]\n",
    "    return None\n",
    "\n",
    "pick_udf = F.udf(pick_candidate, cand_struct)\n",
    "\n",
    "df4 = df3.withColumn(\"es_candidate\", pick_udf(F.col(\"es_candidates\"), F.col(\"target_pos\")))\n",
    "\n",
    "df4.select(\n",
    "    \"target_pos\",\n",
    "    F.col(\"es_candidate.es_id\").alias(\"es_id\"),\n",
    "    F.col(\"es_candidate.es_score\").alias(\"es_score\")\n",
    ").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2081739",
   "metadata": {},
   "source": [
    "## 7) Auditoria: distribuição observada e taxa de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb826196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "aud = (\n",
    "    df4\n",
    "    .withColumn(\"has_candidate\", F.col(\"es_candidate\").isNotNull())\n",
    "    .groupBy(\"target_pos\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n\"),\n",
    "        F.sum(F.col(\"has_candidate\").cast(\"int\")).alias(\"n_with_candidate\")\n",
    "    )\n",
    "    .withColumn(\"pct_rows\", F.col(\"n\") / F.sum(\"n\").over(Window.partitionBy()) * 100)\n",
    "    .withColumn(\"pct_with_candidate\", F.col(\"n_with_candidate\") / F.col(\"n\") * 100)\n",
    "    .orderBy(\"target_pos\")\n",
    ")\n",
    "\n",
    "aud.show(50, truncate=False)\n",
    "\n",
    "df4.select(F.mean(F.col(\"es_candidate\").isNull().cast(\"int\")).alias(\"null_rate\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d51a5a",
   "metadata": {},
   "source": [
    "## 8) Exportar para revisão manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ca101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sugestões:\n",
    "# - manter colunas de linkage + es_candidate (+/- es_candidates)\n",
    "# - salvar em parquet/delta para revisão interna\n",
    "# - ou gerar CSV (cuidado com tamanho de es_candidates)\n",
    "\n",
    "cols_out = [\n",
    "    \"nome\", \"nome_mae\", \"dt_nasc\", \"sexo\", \"municipio_res\",\n",
    "    \"target_pos\",\n",
    "    \"es_candidate\",\n",
    "    \"es_candidates\",\n",
    "]\n",
    "cols_out = [c for c in cols_out if c in df4.columns]\n",
    "\n",
    "df_out = df4.select(*cols_out)\n",
    "df_out.show(5, truncate=False)\n",
    "\n",
    "# Exemplo de escrita:\n",
    "# df_out.write.mode(\"overwrite\").parquet(\"/mnt/data/linkage_manual_review.parquet\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
