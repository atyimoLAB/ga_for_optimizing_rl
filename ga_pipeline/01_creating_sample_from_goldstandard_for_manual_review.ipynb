{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b940478",
   "metadata": {},
   "source": [
    "# Linkage manual review dataset — Elasticsearch candidates with controlled rank sampling\n",
    "\n",
    "Este notebook cria, a partir de um DataFrame Spark com atributos de linkage, uma coluna:\n",
    "\n",
    "- `es_candidates`: lista (array) com até **N** candidatos retornados do Elasticsearch (via `_msearch`)\n",
    "- `target_pos`: posição-alvo sorteada por linha segundo uma **distribuição configurada em YAML**\n",
    "- `es_candidate`: candidato escolhido de `es_candidates[target_pos-1]` com fallback configurável\n",
    "\n",
    "Pré-requisitos:\n",
    "- Cluster Spark disponível\n",
    "- Acesso ao Elasticsearch (rede + credenciais se necessário)\n",
    "- Pacotes Python: `pyyaml`, `requests`\n",
    "\n",
    "> Observação: a abordagem usa `_msearch` para reduzir o overhead (lote por partição).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27839e2d-7a91-43c7-bd73-7fbd609692b5",
   "metadata": {},
   "source": [
    "# Importing libs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68101c03-d2ea-4241-a767-34206c362ac7",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15746eb0-81b4-43b4-870f-d784a3cce4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03c62f-7145-40bd-9dd9-3670acce3062",
   "metadata": {},
   "source": [
    "## Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8b8672-0778-4056-8580-78d77cefac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e0a92f-adbe-4c33-bdbe-b04038908c36",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0328fe-e227-4b2f-a26f-eebb54291afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import yaml\n",
    "from typing import Iterator, Dict, Any\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e8cf8-6457-44b5-8a5c-c542bf696472",
   "metadata": {},
   "source": [
    "# Starting spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8b2fa5-90b1-448e-8ca5-841c8fe9c62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.elasticsearch#elasticsearch-spark-30_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2e0c78bc-6bbe-4fde-ab3b-aa734493facb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.elasticsearch#elasticsearch-spark-30_2.12;8.1.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.6 in central\n",
      "\tfound commons-logging#commons-logging;1.1.1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound org.apache.spark#spark-yarn_2.12;3.2.0 in central\n",
      ":: resolution report :: resolve 280ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-yarn_2.12;3.2.0 from central in [default]\n",
      "\torg.elasticsearch#elasticsearch-spark-30_2.12;8.1.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.6 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2e0c78bc-6bbe-4fde-ab3b-aa734493facb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/5ms)\n",
      "26/01/31 19:59:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/31 19:59:35 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TrainDataSet\") \\\n",
    "    .master(\"spark://barravento:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:8.1.3\") \\\n",
    "    .config(\"spark.es.nodes\", \"barravento,jardimdealah,stellamaris\") \\\n",
    "    .config(\"spark.es.port\", \"9200\") \\\n",
    "    .config(\"spark.es.nodes.wan.only\", \"false\") \\\n",
    "    .config(\"spark.es.resource\", \"db2\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 32) \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"256m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "# just to ensure that \n",
    "sc.setCheckpointDir(\"hdfs://barravento:9000/spark-checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10624d4-321b-4c6f-8028-feaa7e7f1fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://003dd2dc9ac5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://barravento:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TrainDataSet</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://barravento:7077 appName=TrainDataSet>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad90d3c-b792-4136-8b06-afab748a19eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://003dd2dc9ac5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://barravento:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TrainDataSet</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7eff9842afd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4ffbf-3370-41e6-8986-9dbffe0474b5",
   "metadata": {},
   "source": [
    "# 0) Lembrar de indexar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56caa89-c5f3-4013-bad8-83e186d994ef",
   "metadata": {},
   "source": [
    "## Criando 'casca' do indice elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5de382b3-375b-4d11-bf63-c555cb43b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -X DELETE \"http://localhost:9200/basemaior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf0c1c71-37d8-4aa8-9409-146b2c49b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES=\"http://barravento:9200\"\n",
    "DST=\"basemaior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0e760b-555f-4466-8c6d-d9325bd668c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "  \"settings\": {\n",
    "    \"number_of_shards\": 3,\n",
    "    \"number_of_replicas\": 2,\n",
    "    \"refresh_interval\": \"30s\"\n",
    "  }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963fb21-c6bc-4070-9c26-4b976fc00494",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.put(f\"{ES}/{DST}\", json=settings)\n",
    "r.raise_for_status()\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322e844-fa85-41e7-b9ef-ee73f0bd5b09",
   "metadata": {},
   "source": [
    "## Lendo arquivo e indexando usando o spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65634713-ca94-424e-b162-fbf57f6ef1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = spark.read.parquet(\"hdfs://barravento:9000/data/synthetic-dataset-A.parquet\")\n",
    "# df_a.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163402e-4835-476a-83b7-8a98d3dd0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_a.write\n",
    "    .format(\"org.elasticsearch.spark.sql\")\n",
    "    .option(\"es.nodes\", \"barravento,jardimdealah,stellamaris\")\n",
    "    .option(\"es.port\", \"9200\")\n",
    "    .option(\"es.resource\", DST)\n",
    "    .option(\"es.batch.size.entries\", \"500\")\n",
    "    .option(\"es.batch.size.bytes\", \"1mb\")\n",
    "    .option(\"es.batch.write.retry.count\", \"10\")\n",
    "    .option(\"es.batch.write.retry.wait\", \"10s\")\n",
    "    .option(\"es.mapping.id\", \"id_cidacs_a\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46e169-659c-4f97-88ee-b8b2c1768067",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(f\"{ES}/{DST}\")\n",
    "r.raise_for_status()\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9793f2b",
   "metadata": {},
   "source": [
    "## 1) Configuração YAML (exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste o caminho do YAML conforme seu ambiente (Databricks / Jupyter / filesystem)\n",
    "yaml_path = \"01_sampling_cfg.yaml\"  # exemplo Databricks\n",
    "# yaml_path = \"linkage_cfg.yaml\"               # exemplo local\n",
    "print(\"YAML path:\", yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d633c1",
   "metadata": {},
   "source": [
    "## 2) Carregar YAML e validar distribuição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1746220",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = yaml.safe_load(open(yaml_path, \"r\"))\n",
    "\n",
    "dist = cfg[\"sampling\"][\"position_distribution_pct\"]\n",
    "seed = int(cfg[\"sampling\"].get(\"seed\", 2026))\n",
    "\n",
    "# valida soma = 100\n",
    "s = sum(int(v) for v in dist.values())\n",
    "assert s == 100, f\"Distribuição precisa somar 100 (atual: {s})\"\n",
    "\n",
    "max_candidates = int(cfg[\"query\"][\"max_candidates\"])\n",
    "fallback_mode = cfg[\"sampling\"][\"fallback_when_short\"][\"mode\"]\n",
    "\n",
    "print(\"OK! max_candidates =\", max_candidates)\n",
    "print(\"fallback_mode =\", fallback_mode)\n",
    "print(\"seed =\", seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2156edf",
   "metadata": {},
   "source": [
    "## 3) DataFrame de entrada (exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b75346-3923-4ddd-acb2-619740fbaa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = spark.read.parquet(\"hdfs://barravento:9000/data/synthetic-datasets-b-1000.parquet\")\n",
    "# df_b.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760ff5d",
   "metadata": {},
   "source": [
    "## 4) Sortear a posição-alvo `target_pos` conforme o YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = sorted([(int(k), int(v)) for k, v in dist.items()], key=lambda x: x[0])\n",
    "\n",
    "# acumulados em [0,1]\n",
    "cum = []\n",
    "acc = 0\n",
    "for pos, pct in items:\n",
    "    acc += pct\n",
    "    cum.append((pos, acc / 100.0))\n",
    "\n",
    "u = F.rand(seed)\n",
    "\n",
    "case_expr = None\n",
    "for pos, thr in cum:\n",
    "    cond = (u <= F.lit(thr))\n",
    "    case_expr = F.when(cond, F.lit(pos)) if case_expr is None else case_expr.when(cond, F.lit(pos))\n",
    "case_expr = case_expr.otherwise(F.lit(items[-1][0]))\n",
    "\n",
    "df2 = df_b.withColumn(\"target_pos\", case_expr.cast(\"int\"))\n",
    "df2.select(\"target_pos\").groupBy(\"target_pos\").count().orderBy(\"target_pos\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee761c-8a9a-4473-b9d8-f3f8e0c559ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_a.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa0556-7516-4603-984a-0d11b4777486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b52a21",
   "metadata": {},
   "source": [
    "## 5) Buscar candidatos no Elasticsearch via `_msearch` (por partição)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd707f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_hosts = [\"http://barravento:9200/\"]\n",
    "es_index = DST\n",
    "\n",
    "fields_cfg = cfg[\"query\"][\"fields\"]\n",
    "# [{'col': 'nome',\n",
    "#  'es_field': 'nome',\n",
    "#  'type': 'match',\n",
    "#  'fuzziness': 'AUTO',\n",
    "#  'boost': 3.0,\n",
    "#  'operator': 'AND'},\n",
    "# ...}]\n",
    "\n",
    "src_fields = cfg[\"query\"].get(\"source_fields\", [])\n",
    "# ['nome', 'nome_mae', 'dt_nasc', 'sexo', 'municipio_res']\n",
    "\n",
    "def build_es_query(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    should = []\n",
    "    for f in fields_cfg:\n",
    "        col = f[\"col\"]\n",
    "        val = row.get(col)\n",
    "\n",
    "        if val is None or (isinstance(val, str) and val.strip() == \"\"):\n",
    "            continue\n",
    "\n",
    "        qtype = f[\"type\"]\n",
    "        es_field = f[\"es_field\"]\n",
    "        boost = float(f.get(\"boost\", 1.0))\n",
    "\n",
    "        if qtype == \"match\":\n",
    "            clause = {\n",
    "                \"match\": {\n",
    "                    es_field: {\n",
    "                        \"query\": val,\n",
    "                        \"boost\": boost,\n",
    "                        \"operator\": f.get(\"operator\", \"OR\"),\n",
    "                        \"fuzziness\": f.get(\"fuzziness\", \"AUTO\"),\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        elif qtype == \"term\":\n",
    "            clause = {\"term\": {es_field: {\"value\": val, \"boost\": boost}}}\n",
    "        else:\n",
    "            raise ValueError(f\"Tipo não suportado: {qtype}\")\n",
    "\n",
    "        should.append(clause)\n",
    "\n",
    "    return {\n",
    "        \"size\": max_candidates,\n",
    "        \"_source\": src_fields,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": should,\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "cand_schema = T.ArrayType(T.StructType([\n",
    "    T.StructField(\"es_id\", T.StringType(), True),\n",
    "    T.StructField(\"es_score\", T.DoubleType(), True),\n",
    "    T.StructField(\"es_source\", T.StringType(), True),  # JSON compactado (opcional)\n",
    "]))\n",
    "\n",
    "out_schema = T.StructType(df2.schema.fields + [\n",
    "    T.StructField(\"es_candidates\", cand_schema, True),\n",
    "])\n",
    "\n",
    "def fetch_partition(rows: Iterator[Any]) -> Iterator[Any]:\n",
    "    rows_list = list(rows)\n",
    "    if not rows_list:\n",
    "        return iter([])\n",
    "\n",
    "    ndjson_lines = []\n",
    "    for r in rows_list:\n",
    "        d = r.asDict(recursive=True)\n",
    "        header = {\"index\": es_index}\n",
    "        query  = build_es_query(d)\n",
    "        ndjson_lines.append(json.dumps(header))\n",
    "        ndjson_lines.append(json.dumps(query))\n",
    "\n",
    "    body = \"\\n\".join(ndjson_lines) + \"\\n\"\n",
    "\n",
    "    base = es_hosts[0].rstrip(\"/\")\n",
    "    url = f\"{base}/_msearch\"\n",
    "\n",
    "    resp = requests.post(\n",
    "        url,\n",
    "        data=body,\n",
    "        headers={\"Content-Type\": \"application/x-ndjson\"},\n",
    "        timeout=60\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    payload = resp.json()\n",
    "\n",
    "    responses = payload.get(\"responses\", [])\n",
    "    if len(responses) != len(rows_list):\n",
    "        raise RuntimeError(f\"Resposta do ES desalinhada: {len(responses)} != {len(rows_list)}\")\n",
    "\n",
    "    out = []\n",
    "    for r, pr in zip(rows_list, responses):\n",
    "        hits = pr.get(\"hits\", {}).get(\"hits\", [])\n",
    "        cands = []\n",
    "        for h in hits:\n",
    "            cands.append({\n",
    "                \"es_id\": h.get(\"_id\"),\n",
    "                \"es_score\": float(h.get(\"_score\") or 0.0),\n",
    "                \"es_source\": json.dumps(h.get(\"_source\", {}), ensure_ascii=False),\n",
    "            })\n",
    "        out.append(tuple(list(r) + [cands]))\n",
    "\n",
    "    return iter(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1533d4-861c-4514-a9d7-a0e118283d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.rdd.mapPartitions(fetch_partition).toDF(schema=out_schema)\n",
    "df3.select(\"target_pos\", F.size(\"es_candidates\").alias(\"n_cands\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12d3e7",
   "metadata": {},
   "source": [
    "## 6) Criar `es_candidate` escolhendo a posição sorteada (com fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_struct = T.StructType([\n",
    "    T.StructField(\"es_id\", T.StringType(), True),\n",
    "    T.StructField(\"es_score\", T.DoubleType(), True),\n",
    "    T.StructField(\"es_source\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "def pick_candidate(cands, target_pos: int):\n",
    "    if cands is None or len(cands) == 0:\n",
    "        return None\n",
    "    idx = (target_pos or 1) - 1\n",
    "    if idx < len(cands):\n",
    "        return cands[idx]\n",
    "    if fallback_mode == \"last_available\":\n",
    "        return cands[-1]\n",
    "    if fallback_mode == \"first_available\":\n",
    "        return cands[0]\n",
    "    return None\n",
    "\n",
    "pick_udf = F.udf(pick_candidate, cand_struct)\n",
    "\n",
    "df4 = df3.withColumn(\"es_candidate\", pick_udf(F.col(\"es_candidates\"), F.col(\"target_pos\")))\n",
    "\n",
    "df4.select(\n",
    "    \"target_pos\",\n",
    "    F.col(\"es_candidate.es_id\").alias(\"es_id\"),\n",
    "    F.col(\"es_candidate.es_score\").alias(\"es_score\")\n",
    ").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c8e77-20ef-4689-bab7-3b9e8b72ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2081739",
   "metadata": {},
   "source": [
    "## 7) Auditoria: distribuição observada e taxa de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb826196",
   "metadata": {},
   "outputs": [],
   "source": [
    "aud = (\n",
    "    df4\n",
    "    .withColumn(\"has_candidate\", F.col(\"es_candidate\").isNotNull())\n",
    "    .groupBy(\"target_pos\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n\"),\n",
    "        F.sum(F.col(\"has_candidate\").cast(\"int\")).alias(\"n_with_candidate\")\n",
    "    )\n",
    "    .withColumn(\"pct_rows\", F.col(\"n\") / F.sum(\"n\").over(Window.partitionBy()) * 100)\n",
    "    .withColumn(\"pct_with_candidate\", F.col(\"n_with_candidate\") / F.col(\"n\") * 100)\n",
    "    .orderBy(\"target_pos\")\n",
    ")\n",
    "\n",
    "aud.show(50, truncate=False)\n",
    "\n",
    "df4.select(F.mean(F.col(\"es_candidate\").isNull().cast(\"int\")).alias(\"null_rate\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ccb20c-7df6-4e23-910b-aeb16fd6db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = (df4\n",
    "    .withColumn(\"es_candidate_id\", F.col(\"es_candidate.es_id\").cast(\"string\"))\n",
    "    .withColumn(\"es_candidate_score\", F.col(\"es_candidate.es_score\").cast(\"double\"))\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62c195-216a-49ad-9138-ba3a46a71f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df4.join(df_a,\n",
    "                  on=[df4.es_candidate_id == df_a.id_cidacs_a],\n",
    "                  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d7c94-5142-4758-89a6-929b78aa2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Base final tem {df_out.count()} registros\")\n",
    "true_matches = df_out.filter(F.col('id_cidacs_b') == F.col('es_candidate_id')).count()\n",
    "false_matches = df_out.filter(F.col('id_cidacs_b') != F.col('es_candidate_id')).count()\n",
    "print(f\"Destes, {true_matches} são true matches e {false_matches} são falsos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb88b3f-c7a6-466a-83f2-5ad5577336b2",
   "metadata": {},
   "source": [
    "# 8) Escrevendo base de resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb597a-8bea-43a2-af72-249ced5185f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.repartition(16).write.parquet(\"hdfs://barravento:9000/data/df_for_manual_review.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e4261-da64-47b8-b145-c89882a6eeea",
   "metadata": {},
   "source": [
    "# 9) Fazendo algumas inspeções (tentando capturar o \"nivel de dificuldade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832571d9-18c9-481e-8e25-630d9ca5adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monkey patch do NumPy antes do toPandas()\n",
    "if not hasattr(np, \"bool\"):\n",
    "    np.bool = bool  # patch para PySpark antigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9c87e-f9df-47ad-882a-e2ff01fb2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = (\n",
    "    df_out\n",
    "    .select(\n",
    "        F.col(\"es_candidate_score\").cast(\"double\").alias(\"score\"),\n",
    "        (F.col(\"id_cidacs_b\").cast(\"string\") == F.col(\"es_candidate_id\").cast(\"string\")).alias(\"is_true_match\")\n",
    "    )\n",
    "    .where(F.col(\"score\").isNotNull())\n",
    ")\n",
    "\n",
    "# amostra para plot (ajuste a fração)\n",
    "plot_pd = plot_df.sample(False, 0.2, seed=2026).toPandas()\n",
    "\n",
    "true_scores  = plot_pd.loc[plot_pd[\"is_true_match\"] == True,  \"score\"].to_numpy()\n",
    "false_scores = plot_pd.loc[plot_pd[\"is_true_match\"] == False, \"score\"].to_numpy()\n",
    "\n",
    "print(\"sample true:\", true_scores.size, \"sample false:\", false_scores.size)\n",
    "\n",
    "all_scores = plot_pd[\"score\"].to_numpy()\n",
    "\n",
    "# eixo robusto (evita outliers estourando o gráfico)\n",
    "q01, q99 = np.quantile(all_scores, [0.01, 0.99])\n",
    "xmin, xmax = float(q01), float(q99)\n",
    "\n",
    "bins = 50\n",
    "edges = np.linspace(xmin, xmax, bins + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(false_scores, bins=edges, density=True, alpha=0.4, label=f\"False matches (n={false_scores.size})\")\n",
    "plt.hist(true_scores,  bins=edges, density=True, alpha=0.4, label=f\"True matches (n={true_scores.size})\")\n",
    "\n",
    "plt.xlabel(\"es_candidate_score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Score distribution: true vs false matches (density overlay)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5818a-80ba-4b58-a015-f181eab3ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = (\n",
    "    df_out\n",
    "    .select(\n",
    "        F.col(\"es_candidate_score\").cast(\"double\").alias(\"score\"),\n",
    "        (F.col(\"id_cidacs_b\").cast(\"string\") == F.col(\"es_candidate_id\").cast(\"string\")).cast(\"int\").alias(\"is_true_match\")\n",
    "    )\n",
    "    .where(F.col(\"score\").isNotNull())\n",
    ")\n",
    "\n",
    "plot_pd = plot_df.sample(False, 0.2, seed=2026).toPandas()\n",
    "\n",
    "true_scores  = plot_pd.loc[plot_pd[\"is_true_match\"] == 1, \"score\"].to_numpy()\n",
    "false_scores = plot_pd.loc[plot_pd[\"is_true_match\"] == 0, \"score\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c138e-e8b6-4bff-8ef4-8504cb7353e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eixo comum (robusto contra outliers)\n",
    "all_scores = np.concatenate([true_scores, false_scores])\n",
    "xmin, xmax = np.quantile(all_scores, [0.01, 0.99])\n",
    "xs = np.linspace(xmin, xmax, 400)\n",
    "\n",
    "kde_true  = gaussian_kde(true_scores)\n",
    "kde_false = gaussian_kde(false_scores)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(xs, kde_false(xs), color=\"red\",  lw=2, label=f\"False matches (n={false_scores.size})\")\n",
    "plt.plot(xs, kde_true(xs),  color=\"blue\", lw=2, label=f\"True matches (n={true_scores.size})\")\n",
    "\n",
    "plt.xlabel(\"es_candidate_score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"KDE of Elasticsearch candidate score\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a1a90-a4b1-466b-bb08-105224c07fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_hist(scores, bins, xmin, xmax, window=5):\n",
    "    hist, edges = np.histogram(scores, bins=bins, range=(xmin, xmax), density=True)\n",
    "    centers = (edges[:-1] + edges[1:]) / 2\n",
    "    kernel = np.ones(window) / window\n",
    "    smooth = np.convolve(hist, kernel, mode=\"same\")\n",
    "    return centers, smooth\n",
    "\n",
    "all_scores = np.concatenate([true_scores, false_scores])\n",
    "xmin, xmax = np.quantile(all_scores, [0.01, 0.99])\n",
    "\n",
    "bins = 60\n",
    "x_f, y_f = smooth_hist(false_scores, bins, xmin, xmax)\n",
    "x_t, y_t = smooth_hist(true_scores,  bins, xmin, xmax)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x_f, y_f, color=\"red\",  lw=2, label=f\"False matches (n={false_scores.size})\")\n",
    "plt.plot(x_t, y_t, color=\"blue\", lw=2, label=f\"True matches (n={true_scores.size})\")\n",
    "\n",
    "plt.xlabel(\"es_candidate_score\")\n",
    "plt.ylabel(\"Density (smoothed)\")\n",
    "plt.title(\"Smoothed score distributions\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
