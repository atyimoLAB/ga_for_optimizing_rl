{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b940478",
   "metadata": {},
   "source": [
    "# Linkage manual review dataset — Elasticsearch candidates with controlled rank sampling\n",
    "\n",
    "Este notebook cria, a partir de um DataFrame Spark com atributos de linkage, uma coluna:\n",
    "\n",
    "- `es_candidates`: lista (array) com até **N** candidatos retornados do Elasticsearch (via `_msearch`)\n",
    "- `target_pos`: posição-alvo sorteada por linha segundo uma **distribuição configurada em YAML**\n",
    "- `es_candidate`: candidato escolhido de `es_candidates[target_pos-1]` com fallback configurável\n",
    "\n",
    "Pré-requisitos:\n",
    "- Cluster Spark disponível\n",
    "- Acesso ao Elasticsearch (rede + credenciais se necessário)\n",
    "- Pacotes Python: `pyyaml`, `requests`\n",
    "\n",
    "> Observação: a abordagem usa `_msearch` para reduzir o overhead (lote por partição).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27839e2d-7a91-43c7-bd73-7fbd609692b5",
   "metadata": {},
   "source": [
    "# Importing libs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68101c03-d2ea-4241-a767-34206c362ac7",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15746eb0-81b4-43b4-870f-d784a3cce4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03c62f-7145-40bd-9dd9-3670acce3062",
   "metadata": {},
   "source": [
    "## Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a8b8672-0778-4056-8580-78d77cefac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e0a92f-adbe-4c33-bdbe-b04038908c36",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a0328fe-e227-4b2f-a26f-eebb54291afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import yaml\n",
    "from typing import Iterator, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e8cf8-6457-44b5-8a5c-c542bf696472",
   "metadata": {},
   "source": [
    "# Starting spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f8b2fa5-90b1-448e-8ca5-841c8fe9c62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.elasticsearch#elasticsearch-spark-30_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8ebd8e8f-104b-46ba-ae1d-0c4e3fdcffe3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.elasticsearch#elasticsearch-spark-30_2.12;8.1.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.6 in central\n",
      "\tfound commons-logging#commons-logging;1.1.1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound org.apache.spark#spark-yarn_2.12;3.2.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-30_2.12/8.1.3/elasticsearch-spark-30_2.12-8.1.3.jar ...\n",
      "\t[SUCCESSFUL ] org.elasticsearch#elasticsearch-spark-30_2.12;8.1.3!elasticsearch-spark-30_2.12.jar (490ms)\n",
      ":: resolution report :: resolve 14452ms :: artifacts dl 492ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-yarn_2.12;3.2.0 from central in [default]\n",
      "\torg.elasticsearch#elasticsearch-spark-30_2.12;8.1.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.6 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   7   |   7   |   0   ||   1   |   1   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8ebd8e8f-104b-46ba-ae1d-0c4e3fdcffe3\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 0 already retrieved (2066kB/6ms)\n",
      "26/01/30 01:13:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/30 01:13:57 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TrainDataSet\") \\\n",
    "    .master(\"spark://barravento:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:8.1.3\") \\\n",
    "    .config(\"spark.es.nodes\", \"barravento,jardimdealah,stellamaris\") \\\n",
    "    .config(\"spark.es.port\", \"9200\") \\\n",
    "    .config(\"spark.es.nodes.wan.only\", \"false\") \\\n",
    "    .config(\"spark.es.resource\", \"db2\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 16) \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"256m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "# just to ensure that \n",
    "sc.setCheckpointDir(\"hdfs://barravento:9000/spark-checkpoints\"){"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4ffbf-3370-41e6-8986-9dbffe0474b5",
   "metadata": {},
   "source": [
    "# 0) Lembrar de indexar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56caa89-c5f3-4013-bad8-83e186d994ef",
   "metadata": {},
   "source": [
    "## Criando 'casca' do indice elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0c1c71-37d8-4aa8-9409-146b2c49b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES=\"http://barravento:9200\"\n",
    "DST=\"basemaior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0e760b-555f-4466-8c6d-d9325bd668c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "  \"settings\": {\n",
    "    \"number_of_shards\": 3,\n",
    "    \"number_of_replicas\": 2,\n",
    "    \"refresh_interval\": \"30s\"\n",
    "  }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4963fb21-c6bc-4070-9c26-4b976fc00494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'basemaior'}\n"
     ]
    }
   ],
   "source": [
    "r = requests.put(f\"{ES}/{DST}\", json=settings)\n",
    "r.raise_for_status()\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322e844-fa85-41e7-b9ef-ee73f0bd5b09",
   "metadata": {},
   "source": [
    "## Lendo arquivo e indexando usando o spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65634713-ca94-424e-b162-fbf57f6ef1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cidacs_a</th>\n",
       "      <th>nome_a</th>\n",
       "      <th>nome_mae_a</th>\n",
       "      <th>dt_nasc_a</th>\n",
       "      <th>sexo_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>YASMIM VITORIA MATIAS FONSECA</td>\n",
       "      <td>TACIANY DOS SANTOS</td>\n",
       "      <td>20071122</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>PEDRO HENRIQUE MARTINS DE CARVALHO</td>\n",
       "      <td>FRANCILEIDE DOS SANTOS ALVES</td>\n",
       "      <td>20061102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_cidacs_a                              nome_a  \\\n",
       "0           1       YASMIM VITORIA MATIAS FONSECA   \n",
       "1           2  PEDRO HENRIQUE MARTINS DE CARVALHO   \n",
       "\n",
       "                     nome_mae_a dt_nasc_a sexo_a  \n",
       "0            TACIANY DOS SANTOS  20071122      2  \n",
       "1  FRANCILEIDE DOS SANTOS ALVES  20061102      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a = spark.read.parquet(\"hdfs://barravento:9000/data/synthetic-dataset-A.parquet\")\n",
    "df_a.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b163402e-4835-476a-83b7-8a98d3dd0570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "(df_a.write\n",
    "    .format(\"org.elasticsearch.spark.sql\")\n",
    "    .option(\"es.nodes\", \"barravento,jardimdealah,stellamaris\")\n",
    "    .option(\"es.port\", \"9200\")\n",
    "    .option(\"es.resource\", DST)\n",
    "    .option(\"es.batch.size.entries\", \"500\")\n",
    "    .option(\"es.batch.size.bytes\", \"1mb\")\n",
    "    .option(\"es.batch.write.retry.count\", \"10\")\n",
    "    .option(\"es.batch.write.retry.wait\", \"10s\")\n",
    "    .option(\"es.mapping.id\", \"id_cidacs_a\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9793f2b",
   "metadata": {},
   "source": [
    "## 1) Configuração YAML (exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76f2dd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML path: 01_sampling_cfg.yaml\n"
     ]
    }
   ],
   "source": [
    "# Ajuste o caminho do YAML conforme seu ambiente (Databricks / Jupyter / filesystem)\n",
    "yaml_path = \"01_sampling_cfg.yaml\"  # exemplo Databricks\n",
    "# yaml_path = \"linkage_cfg.yaml\"               # exemplo local\n",
    "print(\"YAML path:\", yaml_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d633c1",
   "metadata": {},
   "source": [
    "## 2) Carregar YAML e validar distribuição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1746220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK! max_candidates = 10\n",
      "fallback_mode = last_available\n",
      "seed = 2026\n"
     ]
    }
   ],
   "source": [
    "cfg = yaml.safe_load(open(yaml_path, \"r\"))\n",
    "\n",
    "dist = cfg[\"sampling\"][\"position_distribution_pct\"]\n",
    "seed = int(cfg[\"sampling\"].get(\"seed\", 2026))\n",
    "\n",
    "# valida soma = 100\n",
    "s = sum(int(v) for v in dist.values())\n",
    "assert s == 100, f\"Distribuição precisa somar 100 (atual: {s})\"\n",
    "\n",
    "max_candidates = int(cfg[\"query\"][\"max_candidates\"])\n",
    "fallback_mode = cfg[\"sampling\"][\"fallback_when_short\"][\"mode\"]\n",
    "\n",
    "print(\"OK! max_candidates =\", max_candidates)\n",
    "print(\"fallback_mode =\", fallback_mode)\n",
    "print(\"seed =\", seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2156edf",
   "metadata": {},
   "source": [
    "## 3) DataFrame de entrada (exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87b75346-3923-4ddd-acb2-619740fbaa1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cidacs_b</th>\n",
       "      <th>nome_b</th>\n",
       "      <th>nome_mae_b</th>\n",
       "      <th>dt_nasc_b</th>\n",
       "      <th>sexo_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>788</td>\n",
       "      <td>RUAN CESAR COSTA DE JESUS</td>\n",
       "      <td>JUSSARA CAROLINA R ALBUQUERQUE</td>\n",
       "      <td>20080531</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1261</td>\n",
       "      <td>YASMIN MUNIZ MARCELINO</td>\n",
       "      <td>VERA LUCIA RIBEIRO</td>\n",
       "      <td>20080516</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_cidacs_b                     nome_b                      nome_mae_b  \\\n",
       "0         788  RUAN CESAR COSTA DE JESUS  JUSSARA CAROLINA R ALBUQUERQUE   \n",
       "1        1261     YASMIN MUNIZ MARCELINO              VERA LUCIA RIBEIRO   \n",
       "\n",
       "  dt_nasc_b sexo_b  \n",
       "0  20080531      1  \n",
       "1  20080516      2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b = spark.read.parquet(\"hdfs://barravento:9000/data/synthetic-datasets-b-1000.parquet\")\n",
    "df_b.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760ff5d",
   "metadata": {},
   "source": [
    "## 4) Sortear a posição-alvo `target_pos` conforme o YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7ae495c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|target_pos|count|\n",
      "+----------+-----+\n",
      "|         1|  561|\n",
      "|         2|  326|\n",
      "|         3|  102|\n",
      "|         4|   11|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items = sorted([(int(k), int(v)) for k, v in dist.items()], key=lambda x: x[0])\n",
    "\n",
    "# acumulados em [0,1]\n",
    "cum = []\n",
    "acc = 0\n",
    "for pos, pct in items:\n",
    "    acc += pct\n",
    "    cum.append((pos, acc / 100.0))\n",
    "\n",
    "u = F.rand(seed)\n",
    "\n",
    "case_expr = None\n",
    "for pos, thr in cum:\n",
    "    cond = (u <= F.lit(thr))\n",
    "    case_expr = F.when(cond, F.lit(pos)) if case_expr is None else case_expr.when(cond, F.lit(pos))\n",
    "case_expr = case_expr.otherwise(F.lit(items[-1][0]))\n",
    "\n",
    "df2 = df_b.withColumn(\"target_pos\", case_expr.cast(\"int\"))\n",
    "df2.select(\"target_pos\").groupBy(\"target_pos\").count().orderBy(\"target_pos\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b52a21",
   "metadata": {},
   "source": [
    "## 5) Buscar candidatos no Elasticsearch via `_msearch` (por partição)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cd707f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|target_pos|n_cands|\n",
      "+----------+-------+\n",
      "|1         |10     |\n",
      "|2         |10     |\n",
      "|1         |10     |\n",
      "|2         |10     |\n",
      "|1         |10     |\n",
      "|1         |10     |\n",
      "|2         |10     |\n",
      "|3         |10     |\n",
      "|2         |10     |\n",
      "|1         |10     |\n",
      "+----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "es_hosts = [\"http://barravento:9200/\"]\n",
    "es_index = DST\n",
    "\n",
    "fields_cfg = cfg[\"query\"][\"fields\"]\n",
    "src_fields = cfg[\"query\"].get(\"source_fields\", [])\n",
    "\n",
    "def build_es_query(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    should = []\n",
    "    for f in fields_cfg:\n",
    "        col = f[\"col\"]\n",
    "        val = row.get(col)\n",
    "\n",
    "        if val is None or (isinstance(val, str) and val.strip() == \"\"):\n",
    "            continue\n",
    "\n",
    "        qtype = f[\"type\"]\n",
    "        es_field = f[\"es_field\"]\n",
    "        boost = float(f.get(\"boost\", 1.0))\n",
    "\n",
    "        if qtype == \"match\":\n",
    "            clause = {\n",
    "                \"match\": {\n",
    "                    es_field: {\n",
    "                        \"query\": val,\n",
    "                        \"boost\": boost,\n",
    "                        \"operator\": f.get(\"operator\", \"OR\"),\n",
    "                        \"fuzziness\": f.get(\"fuzziness\", \"AUTO\"),\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        elif qtype == \"term\":\n",
    "            clause = {\"term\": {es_field: {\"value\": val, \"boost\": boost}}}\n",
    "        else:\n",
    "            raise ValueError(f\"Tipo não suportado: {qtype}\")\n",
    "\n",
    "        should.append(clause)\n",
    "\n",
    "    return {\n",
    "        \"size\": max_candidates,\n",
    "        \"_source\": src_fields,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": should,\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "cand_schema = T.ArrayType(T.StructType([\n",
    "    T.StructField(\"es_id\", T.StringType(), True),\n",
    "    T.StructField(\"es_score\", T.DoubleType(), True),\n",
    "    T.StructField(\"es_source\", T.StringType(), True),  # JSON compactado (opcional)\n",
    "]))\n",
    "\n",
    "out_schema = T.StructType(df2.schema.fields + [\n",
    "    T.StructField(\"es_candidates\", cand_schema, True),\n",
    "])\n",
    "\n",
    "def fetch_partition(rows: Iterator[Any]) -> Iterator[Any]:\n",
    "    rows_list = list(rows)\n",
    "    if not rows_list:\n",
    "        return iter([])\n",
    "\n",
    "    ndjson_lines = []\n",
    "    for r in rows_list:\n",
    "        d = r.asDict(recursive=True)\n",
    "        header = {\"index\": es_index}\n",
    "        query  = build_es_query(d)\n",
    "        ndjson_lines.append(json.dumps(header))\n",
    "        ndjson_lines.append(json.dumps(query))\n",
    "\n",
    "    body = \"\\n\".join(ndjson_lines) + \"\\n\"\n",
    "\n",
    "    base = es_hosts[0].rstrip(\"/\")\n",
    "    url = f\"{base}/_msearch\"\n",
    "\n",
    "    resp = requests.post(\n",
    "        url,\n",
    "        data=body,\n",
    "        headers={\"Content-Type\": \"application/x-ndjson\"},\n",
    "        timeout=60\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    payload = resp.json()\n",
    "\n",
    "    responses = payload.get(\"responses\", [])\n",
    "    if len(responses) != len(rows_list):\n",
    "        raise RuntimeError(f\"Resposta do ES desalinhada: {len(responses)} != {len(rows_list)}\")\n",
    "\n",
    "    out = []\n",
    "    for r, pr in zip(rows_list, responses):\n",
    "        hits = pr.get(\"hits\", {}).get(\"hits\", [])\n",
    "        cands = []\n",
    "        for h in hits:\n",
    "            cands.append({\n",
    "                \"es_id\": h.get(\"_id\"),\n",
    "                \"es_score\": float(h.get(\"_score\") or 0.0),\n",
    "                \"es_source\": json.dumps(h.get(\"_source\", {}), ensure_ascii=False),\n",
    "            })\n",
    "        out.append(tuple(list(r) + [cands]))\n",
    "\n",
    "    return iter(out)\n",
    "\n",
    "df3 = df2.rdd.mapPartitions(fetch_partition).toDF(schema=out_schema)\n",
    "\n",
    "df3.select(\"target_pos\", F.size(\"es_candidates\").alias(\"n_cands\")).show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12d3e7",
   "metadata": {},
   "source": [
    "## 6) Criar `es_candidate` escolhendo a posição sorteada (com fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "177c2a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+\n",
      "|target_pos|es_id |es_score|\n",
      "+----------+------+--------+\n",
      "|1         |946044|1.0     |\n",
      "|2         |946048|1.0     |\n",
      "|1         |946044|1.0     |\n",
      "|2         |946048|1.0     |\n",
      "|1         |946044|1.0     |\n",
      "|1         |946044|1.0     |\n",
      "|2         |946048|1.0     |\n",
      "|3         |946049|1.0     |\n",
      "|2         |946048|1.0     |\n",
      "|1         |946044|1.0     |\n",
      "|2         |946048|1.0     |\n",
      "|2         |946048|1.0     |\n",
      "|3         |946049|1.0     |\n",
      "|1         |946044|1.0     |\n",
      "|1         |946044|1.0     |\n",
      "|1         |946044|1.0     |\n",
      "|1         |946044|1.0     |\n",
      "|1         |946044|1.0     |\n",
      "|2         |946048|1.0     |\n",
      "|1         |946044|1.0     |\n",
      "+----------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "cand_struct = T.StructType([\n",
    "    T.StructField(\"es_id\", T.StringType(), True),\n",
    "    T.StructField(\"es_score\", T.DoubleType(), True),\n",
    "    T.StructField(\"es_source\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "def pick_candidate(cands, target_pos: int):\n",
    "    if cands is None or len(cands) == 0:\n",
    "        return None\n",
    "    idx = (target_pos or 1) - 1\n",
    "    if idx < len(cands):\n",
    "        return cands[idx]\n",
    "    if fallback_mode == \"last_available\":\n",
    "        return cands[-1]\n",
    "    if fallback_mode == \"first_available\":\n",
    "        return cands[0]\n",
    "    return None\n",
    "\n",
    "pick_udf = F.udf(pick_candidate, cand_struct)\n",
    "\n",
    "df4 = df3.withColumn(\"es_candidate\", pick_udf(F.col(\"es_candidates\"), F.col(\"target_pos\")))\n",
    "\n",
    "df4.select(\n",
    "    \"target_pos\",\n",
    "    F.col(\"es_candidate.es_id\").alias(\"es_id\"),\n",
    "    F.col(\"es_candidate.es_score\").alias(\"es_score\")\n",
    ").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2081739",
   "metadata": {},
   "source": [
    "## 7) Auditoria: distribuição observada e taxa de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb826196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "aud = (\n",
    "    df4\n",
    "    .withColumn(\"has_candidate\", F.col(\"es_candidate\").isNotNull())\n",
    "    .groupBy(\"target_pos\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n\"),\n",
    "        F.sum(F.col(\"has_candidate\").cast(\"int\")).alias(\"n_with_candidate\")\n",
    "    )\n",
    "    .withColumn(\"pct_rows\", F.col(\"n\") / F.sum(\"n\").over(Window.partitionBy()) * 100)\n",
    "    .withColumn(\"pct_with_candidate\", F.col(\"n_with_candidate\") / F.col(\"n\") * 100)\n",
    "    .orderBy(\"target_pos\")\n",
    ")\n",
    "\n",
    "aud.show(50, truncate=False)\n",
    "\n",
    "df4.select(F.mean(F.col(\"es_candidate\").isNull().cast(\"int\")).alias(\"null_rate\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d51a5a",
   "metadata": {},
   "source": [
    "## 8) Exportar para revisão manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ca101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sugestões:\n",
    "# - manter colunas de linkage + es_candidate (+/- es_candidates)\n",
    "# - salvar em parquet/delta para revisão interna\n",
    "# - ou gerar CSV (cuidado com tamanho de es_candidates)\n",
    "\n",
    "cols_out = [\n",
    "    \"nome\", \"nome_mae\", \"dt_nasc\", \"sexo\", \"municipio_res\",\n",
    "    \"target_pos\",\n",
    "    \"es_candidate\",\n",
    "    \"es_candidates\",\n",
    "]\n",
    "cols_out = [c for c in cols_out if c in df4.columns]\n",
    "\n",
    "df_out = df4.select(*cols_out)\n",
    "df_out.show(5, truncate=False)\n",
    "\n",
    "# Exemplo de escrita:\n",
    "# df_out.write.mode(\"overwrite\").parquet(\"/mnt/data/linkage_manual_review.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
