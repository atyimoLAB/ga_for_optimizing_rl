{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929b6f68",
   "metadata": {},
   "source": [
    "# Spark-native Similarity Scoring with Parameter Sampling\n",
    "\n",
    "This notebook merges the optimized Spark solution with parameter sampling so that **each row in the metrics table corresponds to one sampled parameter set**.\n",
    "\n",
    "Key properties:\n",
    "- No `toPandas()` in the loop (avoids Java heap space errors)\n",
    "- Similarities computed **once** and cached\n",
    "- Weights/penalties applied per configuration using Spark expressions\n",
    "- Metrics aggregated in Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import jellyfish\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da443583",
   "metadata": {},
   "source": [
    "## Base Similarity UDFs (raw, unweighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35903779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jw_raw(col1, col2):\n",
    "    if col1 is None or col2 is None or col1 == '' or col2 == '':\n",
    "        return None\n",
    "    return float(jellyfish.jaro_winkler_similarity(str(col1), str(col2)))\n",
    "\n",
    "def hamming_raw(col1, col2):\n",
    "    if col1 is None or col2 is None or col1 == '' or col2 == '':\n",
    "        return None\n",
    "    s1, s2 = str(col1), str(col2)\n",
    "    max_size = max(len(s1), len(s2))\n",
    "    if max_size == 0:\n",
    "        return None\n",
    "    return 1.0 - (float(jellyfish.hamming_distance(s1, s2)) / max_size)\n",
    "\n",
    "def overlap_raw(col1, col2):\n",
    "    if col1 is None or col2 is None or col1 == '' or col2 == '':\n",
    "        return None\n",
    "    return 1.0 if str(col1) == str(col2) else 0.0\n",
    "\n",
    "udf_jw_raw = F.udf(jw_raw, T.DoubleType())\n",
    "udf_hamming_raw = F.udf(hamming_raw, T.DoubleType())\n",
    "udf_overlap_raw = F.udf(overlap_raw, T.DoubleType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5deb48a",
   "metadata": {},
   "source": [
    "## Compute and Cache Base Similarities (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923fc933",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = (\n",
    "    link_df\n",
    "    .withColumn('jw_nome_raw', udf_jw_raw(F.col('nome_a'), F.col('nome_b')))\n",
    "    .withColumn('jw_nome_mae_raw', udf_jw_raw(F.col('nome_mae_a'), F.col('nome_mae_b')))\n",
    "    .withColumn('ham_dt_nasc_raw', udf_hamming_raw(F.col('dt_nasc_a'), F.col('dt_nasc_b')))\n",
    "    .withColumn('ov_sexo_raw', udf_overlap_raw(F.col('sexo_a'), F.col('sexo_b')))\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "base_df.count()  # materialize cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec66ab8",
   "metadata": {},
   "source": [
    "## Parameter Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf1c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_param_sets(cfg, n, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    fields = cfg['dataset']['fields']\n",
    "    rows = []\n",
    "    for _ in range(n):\n",
    "        r = {}\n",
    "        for k, spec in fields.items():\n",
    "            w = spec['weight']\n",
    "            p = spec['penalty']\n",
    "            w_vals = np.arange(w['low'], w['high'] + 1e-9, w['step'])\n",
    "            p_vals = np.arange(p['low'], p['high'] + 1e-9, p['step'])\n",
    "            r[f'w_{k}'] = float(rng.choice(w_vals))\n",
    "            r[f'p_{k}'] = float(rng.choice(p_vals))\n",
    "        rows.append(r)\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e050b61",
   "metadata": {},
   "source": [
    "## Apply Parameters and Compute Total Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f78f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_params(df, params):\n",
    "    return (\n",
    "        df\n",
    "        .withColumn('sim_nome', F.when(F.col('jw_nome_raw').isNull(), F.lit(params['p_nome']))\n",
    "                               .otherwise(F.col('jw_nome_raw') * F.lit(params['w_nome'])))\n",
    "        .withColumn('sim_nome_mae', F.when(F.col('jw_nome_mae_raw').isNull(), F.lit(params['p_nome_mae']))\n",
    "                                   .otherwise(F.col('jw_nome_mae_raw') * F.lit(params['w_nome_mae'])))\n",
    "        .withColumn('sim_dt_nasc', F.when(F.col('ham_dt_nasc_raw').isNull(), F.lit(params['p_dt_nasc']))\n",
    "                                 .otherwise(F.col('ham_dt_nasc_raw') * F.lit(params['w_dt_nasc'])))\n",
    "        .withColumn('sim_sexo', F.when(F.col('ov_sexo_raw').isNull(), F.lit(params['p_sexo']))\n",
    "                              .otherwise(F.col('ov_sexo_raw') * F.lit(params['w_sexo'])))\n",
    "    )\n",
    "\n",
    "def with_total_score(df, params):\n",
    "    score_max = params['w_nome'] + params['w_nome_mae'] + params['w_dt_nasc'] + params['w_sexo']\n",
    "    return df.withColumn('total_score',\n",
    "                         (F.col('sim_nome') + F.col('sim_nome_mae') + F.col('sim_dt_nasc') + F.col('sim_sexo')) / F.lit(score_max))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e67cc",
   "metadata": {},
   "source": [
    "## Metrics per Parameter Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee777a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df, threshold):\n",
    "    agg = df.agg(\n",
    "        F.sum(F.when((F.col('match_status') == 1) & (F.col('total_score') >= threshold), 1).otherwise(0)).alias('VP'),\n",
    "        F.sum(F.when((F.col('match_status') == 0) & (F.col('total_score') >= threshold), 1).otherwise(0)).alias('FP'),\n",
    "        F.sum(F.when((F.col('match_status') == 1) & (F.col('total_score') < threshold), 1).otherwise(0)).alias('FN'),\n",
    "        F.sum(F.when((F.col('match_status') == 0) & (F.col('total_score') < threshold), 1).otherwise(0)).alias('VN'),\n",
    "    )\n",
    "    return (\n",
    "        agg\n",
    "        .withColumn('precision', F.col('VP') / (F.col('VP') + F.col('FP')))\n",
    "        .withColumn('recall', F.col('VP') / (F.col('VP') + F.col('FN')))\n",
    "        .withColumn('specificity', F.col('VN') / (F.col('VN') + F.col('FP')))\n",
    "        .withColumn('accuracy', (F.col('VP') + F.col('VN')) /\n",
    "                              (F.col('VP') + F.col('FP') + F.col('FN') + F.col('VN')))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3392b3a",
   "metadata": {},
   "source": [
    "## Experiment Loop (each row = one parameter set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963babbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = sample_param_sets(cfg, n=1000, seed=7)\n",
    "metrics_rows = []\n",
    "for params in rows:\n",
    "    scored = apply_params(base_df, params)\n",
    "    scored = with_total_score(scored, params)\n",
    "    m = compute_metrics(scored, threshold=0.8)\n",
    "    for k, v in params.items():\n",
    "        m = m.withColumn(k, F.lit(v))\n",
    "    metrics_rows.append(m)\n",
    "\n",
    "metrics_df = metrics_rows[0]\n",
    "for m in metrics_rows[1:]:\n",
    "    metrics_df = metrics_df.unionByName(m)\n",
    "\n",
    "metrics_df.show(5)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
